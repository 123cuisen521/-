爬虫文件
＃ -  *  - 编码：utf-8  -  *  - 
import scrapy


class LianjiaSpider(scrapy.Spider):
    name = 'lianjia'
    allowed_domains = ['ty.lianjia.com']
    start_urls = ['https://ty.lianjia.com/ershoufang/pg{}/'.format(num) for num in range(1,101)]

    def parse(self, response):
        all_url = response.xpath('//div[@class="info clear"]/div[1]/a/@href').extract()
        for url in all_url:
            yield scrapy.Request(url,callback=self.parse_info)

    def parse_info(self,response):
        XQM = response.xpath('//div[@class="communityName"]/a[1]/text()').extract()
        FJ = response.xpath('concat(//span[@class="total"]/text(),//span[@class="unit"]/span/text())').extract()
        QM = response.xpath('string(//div[@class="areaName"]/span[2])').extract()

        HX = response.xpath('//div[@class="base"]/div[2]/ul/li[1]/text()').extract()
        FWMJ = response.xpath('//div[@class="base"]/div[2]/ul/li[3]/text()').extract()
        FWCX = response.xpath('//div[@class="base"]/div[2]/ul/li[7]/text()').extract()
        CQNX = response.xpath('//div[@class="base"]/div[2]/ul/li[12]/text()').extract()
        PBDT = response.xpath('//div[@class="base"]/div[2]/ul/li[11]/text()').extract()
        SZLC = response.xpath('//div[@class="base"]/div[2]/ul/li[2]/text()').extract()
        ZX = response.xpath('//div[@class="base"]/div[2]/ul/li[9]/text()').extract()
        DYXI = response.xpath('//div[@class="transaction"]/div[2]/ul/li[7]/span[2]/text()').extract_first().strip()


        for XQM,FJ,QM,HX,FWMJ,FWCX,CQNX,PBDT,SZLC,ZX,DYXI in zip(XQM,FJ,QM,HX,FWMJ,FWCX,CQNX,PBDT,SZLC,ZX,DYXI):
            yield {
                'QM':QM,
                'XQM':XQM,
                'FJ':FJ,
                'CQNX':CQNX,
                'HX':HX,
                'FWMJ':FWMJ,
                'FWCX':FWCX,
                'PBDT':PBDT,
                'SZLC':SZLC,
                'ZX':ZX,
                'DYXI':DYXI
            }
